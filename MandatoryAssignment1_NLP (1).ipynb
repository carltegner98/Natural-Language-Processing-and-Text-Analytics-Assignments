{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copenhagen Business School\n",
        "\n",
        "MSc in Business Administration and Data Science\n",
        "\n",
        "Natural Language Processing and Text Analytics\n",
        "\n",
        "Mandatory Assignment 1\n",
        "\n",
        "Students:\n",
        "\n",
        "- Joel Berglund - 167681\n",
        "\n",
        "- William Keller Boll - 167326\n",
        "\n",
        "- Eirik Lile Vågen - 167300\n",
        "\n",
        "- Carl Tegnér - 167302"
      ],
      "metadata": {
        "id": "aJ9lThVWPPIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 1\n",
        "The following two sentences are ambiguous – that is, they each have two (or more) different readings.\n",
        "For each sentence, explain the different readings:\n",
        "1. The girl attacked the boy with the book.\n",
        "2. We decided to leave on Saturday.\n",
        "\n",
        "Are the following sentences ambiguous? If the sentence is ambiguous, explain the different readings. If not, explain why not.\n",
        "\n",
        "3. I saw a man with a briefcase.\n",
        "4. I saw the planet with a telescope.\n",
        "\n",
        "Try the above sentences (3 and 4) in this parse tree generator:\n",
        "https://huggingface.co/spaces/nanom/syntactic_tree\n",
        "Compare the dependency structures for both of them. Explain the differences in the link to the\n",
        "preposition “with” in 3 vs. 4."
      ],
      "metadata": {
        "id": "O-WrE04h1O57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence 1: The girl attacked the boy with the book.\n",
        "\n",
        "Sentence 2: We decided to leave on Saturday.\n",
        "\n",
        "The two sentences above are ambiguous. They can be understood in more than just one single way. If we start off with the first example, “with the book” is the part that leaves the understanding of the sentence ambiguous. We would argue that there are two main interpretations of this sentence. The first being that the girl attacks the boy using a book, physically using the book to hit the boy. The second interpretation would be that the girl attacks the boy, and the book being in his possession is what is used to describe the boy.\n",
        "\n",
        "The second sentence is ambiguous around the on Saturday part. The first interpretation would be that they (We) had, at an earlier point in time before the sentence was uttered, decided to leave the on the following Saturday from when the sentence was uttered. The second interpretation could be that the decision to leave was decided the last Saturday from when the sentence was uttered.\n",
        "\n",
        "The following two sentences both use with as a preposition, but in different ways. After this section, the sentences and their dependency structures can be seen. The sentences are as follows “I saw a man with a briefcase.” and “I saw the planet with a telescope.”. In the first sentence, with is used to describe the man, like in the second interpretation of sentence 1, where the book is used to describe the boy. We would argue that it is virtually unambiguous, as everyone would understand that a briefcase is used to describe the man, and not used as a tool to see the man.\n",
        "\n",
        "The same goes for the last sentence. In this sentence, with is used as a preposition to describe how the subject saw a planet. The telescope was a tool that facilitated and allowed the subject to see the planet. One could argue that one, in very special cases, could interpret the sentence as a planet being identified as the one with a telescope. But virtually everyone would interpret the sentence as the subject using a telescope to see the planet.\n",
        "\n",
        "\n",
        "Sentence 3: I saw a man with a briefcase.\n",
        "\n",
        "saw: (Root) (VBD)\n",
        "\n",
        "├── I: 1 (PRP)\n",
        "\n",
        "├── man: 1 (NN)\n",
        "\n",
        "│   ├── a: 2 (DT)\n",
        "\n",
        "│   └── with: 2 (IN)\n",
        "\n",
        "│       └── briefcase: 3 (NN)\n",
        "\n",
        "│           └── a: 4 (DT)\n",
        "\n",
        "└── .: 1 (.)\n",
        "\n",
        "Sentence 4: I saw the planet with a telescope.\n",
        "\n",
        "saw: (Root) (VBD)\n",
        "\n",
        "├── I: 1 (PRP)\n",
        "\n",
        "├── planet: 1 (NN)\n",
        "\n",
        "│   └── the: 2 (DT)\n",
        "\n",
        "├── with: 1 (IN)\n",
        "\n",
        "│   └── telescope: 2 (NN)\n",
        "\n",
        "│       └── a: 3 (DT)\n",
        "\n",
        "└── .: 1 (.)\n",
        "\n"
      ],
      "metadata": {
        "id": "e0kEucje1Z5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 2 - [Linguistic Analysis of a Text Corpus Using spaCy]\n",
        "\n"
      ],
      "metadata": {
        "id": "M9TKbsCRykCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk import pos_tag\n",
        "from prettytable import PrettyTable\n",
        "import spacy.cli\n",
        "from collections import Counter\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download the spaCy English model, if not already installed\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06FFA6gl-j1L",
        "outputId": "84888f20-ae4f-44cd-b57f-ec116113a93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Prepare the Corpus: Pre-process the corpus data by removing stopwords, removing extra\n",
        "spaces, converting all text to lowercase, or apply any other text normalization techniques that\n",
        "you think is relevant.\n",
        "\n",
        "2. POS Tagging and NER: Apply POS tagging to the entire corpus to analyze the distribution of\n",
        "different parts of speech. Use NER to identify and categorize named entities within the text\n",
        "such as name, date, locations etc."
      ],
      "metadata": {
        "id": "C13sbqR0APQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your custom stop words\n",
        "custom_stop_words = ['!', \"'\", \"''\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", ',', '--', '.', '112._', '1920',\n",
        "                    '26._', '45._', '5._', ':', ';', '?', '[', ']', '_71_', '_could_', '_had_', '_has_', '_him_', '_his_',\n",
        "                    \"_i'd_\", '_our_', '_page', '_their_', '_was_', '``', \"n't\"]\n",
        "\n",
        "# Add custom stopwords to NLTK's list\n",
        "stop_words = set(stopwords.words('english')) | set(custom_stop_words)\n",
        "\n",
        "\n",
        "\n",
        "file_path_drive = '/content/drive/MyDrive/CBS/Övningsuppgifter T2/NLP/sample.txt'  # Path for Google Colab\n",
        "file_path_local = 'sample.txt'  # Local file path for Jupyter Notebook\n",
        "\n",
        "try:\n",
        "    # Try to open the file assuming it's in Google Drive\n",
        "    with open(file_path_drive, 'r', encoding='utf-8') as file:\n",
        "        text = file.read().lower()\n",
        "except FileNotFoundError:\n",
        "    # If the file is not found, try to open it from a local path\n",
        "    try:\n",
        "        with open(file_path_local, 'r', encoding='utf-8') as file:\n",
        "            text = file.read().lower()\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found. Please check your file path.\")\n",
        "\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Filter out stop words and non-alphabetic tokens\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
        "\n",
        "\n",
        "# Generate frequency distribution\n",
        "freq_dist = FreqDist(filtered_tokens)\n",
        "# Sort by frequency (already in descending order)\n",
        "sorted_by_freq = sorted(freq_dist.items(), key=lambda item: item[1], reverse=True)\n",
        "print(sorted_by_freq)\n",
        "\n",
        "# Apply POS tagging to filtered tokens\n",
        "pos_tags = pos_tag([word for word, freq in sorted_by_freq])\n",
        "\n",
        "# Create a dictionary for quick POS tag lookup\n",
        "pos_tags_dict = {word: tag for word, tag in pos_tags}\n",
        "\n",
        "# Process the text with spaCy for NER, create a dictionary for quick NER lookup\n",
        "doc = nlp(' '.join(filtered_tokens))\n",
        "ner_dict = {ent.text: ent.label_ for ent in doc.ents}\n",
        "\n",
        "# Initialize PrettyTable\n",
        "table = PrettyTable(['Word', 'Freq', 'POS', 'NER'])\n",
        "\n",
        "# Initialize a list for words with a named entity tag\n",
        "words_with_ne = []\n",
        "\n",
        "# Fill the table with data and prepare words_with_ne list\n",
        "for word, freq in sorted_by_freq:\n",
        "    pos_tag = pos_tags_dict.get(word, 'N/A')\n",
        "    ne_tag = ner_dict.get(word, 'None')  # Default to 'None' if word is not a named entity\n",
        "    table.add_row([word, freq, pos_tag, ne_tag])\n",
        "\n",
        "    # If the word has a NE tag other than 'None', add it to words_with_ne list\n",
        "    if ne_tag != 'None':\n",
        "        words_with_ne.append((word, ne_tag))\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "id": "MSK7SpPY-yod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Analysis: Determine the most common POS tags and discuss what this reveals about the structure of the text."
      ],
      "metadata": {
        "id": "xjzdBvGiABrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the frequency of each POS tag\n",
        "pos_tags_freq = Counter([tag for word, tag in pos_tags])\n",
        "\n",
        "# Count the frequency of each NER tag\n",
        "ner_tags_freq = Counter(ner_dict.values())\n",
        "\n",
        "# Print the frequencies of each POS tag in descending order\n",
        "print(\"POS Tag Frequencies (Descending Order):\")\n",
        "for tag, freq in pos_tags_freq.most_common():\n",
        "    print(f\"{tag}: {freq}\")\n",
        "\n",
        "print(\"\\nNER Tag Frequencies (Descending Order):\")\n",
        "for tag, freq in ner_tags_freq.most_common():\n",
        "    print(f\"{tag}: {freq}\")"
      ],
      "metadata": {
        "id": "DjoH5Ydt0vfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4feb289a-d58e-485b-fd2b-63538213146b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tag Frequencies (Descending Order):\n",
            "NN: 373\n",
            "JJ: 279\n",
            "VBD: 127\n",
            "RB: 123\n",
            "NNS: 100\n",
            "VBG: 91\n",
            "VBN: 78\n",
            "VBP: 42\n",
            "VB: 25\n",
            "VBZ: 24\n",
            "JJS: 23\n",
            "IN: 20\n",
            "JJR: 12\n",
            "MD: 7\n",
            "RBR: 5\n",
            "NNP: 5\n",
            "CD: 3\n",
            "DT: 3\n",
            "CC: 2\n",
            "RP: 1\n",
            "WP$: 1\n",
            "PRP: 1\n",
            "WP: 1\n",
            "FW: 1\n",
            "WRB: 1\n",
            "WDT: 1\n",
            "\n",
            "NER Tag Frequencies (Descending Order):\n",
            "PERSON: 32\n",
            "TIME: 10\n",
            "DATE: 9\n",
            "ORG: 7\n",
            "CARDINAL: 4\n",
            "ORDINAL: 2\n",
            "NORP: 1\n",
            "QUANTITY: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The high frequency of nouns (NN: 373, NNS: 100, NNP: 5) and adjectives (JJ: 279, JJR: 12, JJS: 23) indicates a descriptive text, possibly narrative, focused on detailing specific subjects, objects, and their characteristics. The variety of verb forms, including the past tense (VBD: 127), gerunds/participles (VBG: 91), past participles (VBN: 78), present tense (VBP: 42, VBZ: 24), and the base form (VB: 25), suggests a mix of actions and states, possibly recounting events or describing ongoing situations."
      ],
      "metadata": {
        "id": "Q05COoXNADN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Prepare a short report: Summarize your findings in a brief report of 1-2 page, including any\n",
        "interesting patterns or insights gained from the POS and NER analysis."
      ],
      "metadata": {
        "id": "PDC80nHP1Fum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From our POS and NER analysis, we have discovered the following. Our NER analysis revealed that the text places a strong emphasis on personal entities (PERSON: 32), temporal references (TIME: 10), (DATE: 9), and organizations (ORG: 7). This suggests that the text narrates human-centric stories and involves elements of a social or institutional context, for instance, a government institution or a social organization, where the characters are engaged. An example of these NER tags could be, \"Michael Johnsson (PERSON) embarked on a journey in the early spring (DATE) of 1923 (DATE).\"\n",
        "\n",
        "However, our NER analysis also showed that occurrences of cardinal (CARDINAL: 4), ordinal (ORDINAL: 2), and quantity (QUANTITY: 1) are less frequent in the text. These NER tags are more commonly found, for example, in recipes or perhaps in sports commentary, where they might discuss, \"on his third attempt, he broke the world record.\"\n",
        "\n",
        "Our POS analysis reveals a high use of nouns (NN: 373, NNS: 100, NNP: 5) and adjectives (JJ: 279, JJR: 12, JJS: 23), indicating the text is descriptive or narrative, with detailed descriptions of states or entities. Additionally, the analysis reveals that the occurrences of verbs in various forms (VBD: 127, VBG: 91, VBN: 78, VBP: 42, VB: 25, VBZ: 24) indicate a dynamic narrative with a mix of actions. An example of this could be, \"She whispered (VBD) softly, her fingers tracing (VBG) the rough texture of the old photograph (NN).\"\n",
        "\n",
        "Furthermore, the POS analysis also suggests that the frequency of prepositions (IN: 20), conjunctions (CC: 2), and determiners (DT: 3) indicate a more straightforward narrative style. For example, a lower frequency might lead to sentences that lack temporal context, for instance: \"He walked the forest.\" versus \"He walked through the forest,\" or \"She works night.\" versus \"She works at night.\" However, this conclusion is not definitive since the text is not extensive, and the number of prepositions present isn't substantial.\n",
        "\n",
        "To conclude our analysis reveals that the text is a story of interpersonal relationships portrayed through descriptions, all inside a societal or institutional setting. The narrative paints its scenes more with words than with numbers, prioritizing the exploration of personal tales above numerical facts. It is, in short, a human-centered storytelling that is rich in action."
      ],
      "metadata": {
        "id": "7oCMz64tH4pp"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}